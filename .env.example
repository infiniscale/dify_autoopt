# 环境变量示例文件
#
# 使用方法：
# 1. 复制此文件到项目根目录
#    cp .env.example .env
# 2. 填写实际的配置值
# 3. 确保 .env 已添加到 .gitignore（不要提交到 Git）

# ===== Dify API 配置 =====
# Dify 主要 API Token（必填）
DIFY_API_TOKEN=your_dify_api_token_here

# Dify 备用 Tokens（可选，用于轮换）
DIFY_FALLBACK_TOKEN_1=your_fallback_token_1
DIFY_FALLBACK_TOKEN_2=your_fallback_token_2

# ===== 评估模型配置 =====
# OpenAI API Key（如果使用 OpenAI 作为评估模型）
EVALUATOR_API_KEY=your_openai_api_key_here

# Anthropic API Key（如果使用 Claude 作为评估模型）
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Azure OpenAI 配置（如果使用 Azure）
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
# AZURE_OPENAI_API_KEY=your_azure_api_key_here
# AZURE_OPENAI_DEPLOYMENT=gpt-4

# ===== 路径配置（可选） =====
# 如果需要覆盖默认路径，可以在这里设置
# OUTPUT_DIR=/var/output
# LOGS_DIR=/var/logs
# WORKFLOW_DSL_DIR=/path/to/workflows

# ===== 日志级别（可选） =====
# LOG_LEVEL=INFO  # DEBUG | INFO | WARNING | ERROR | CRITICAL

# ===== 其他配置（可选） =====
# 数据库连接（如果使用数据库存储结果）
# DATABASE_URL=postgresql://user:password@localhost/dbname

# Redis 连接（如果使用缓存）
# REDIS_URL=redis://localhost:6379/0

# ===== 开发环境特定配置 =====
# 开发环境标识
# ENVIRONMENT=dev  # dev | test | staging | prod

# 是否启用调试模式
# DEBUG=false

# ===== LLM Configuration for Optimizer Module =====
# LLM Provider (openai, anthropic, local, stub)
# stub = rule-based optimization without LLM (default, no API costs)
LLM_PROVIDER=stub

# OpenAI API Key (Required if LLM_PROVIDER=openai)
OPENAI_API_KEY=sk-your-openai-api-key-here

# Anthropic API Key (Required if LLM_PROVIDER=anthropic)
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# LLM Model Selection
# OpenAI: gpt-4-turbo-preview, gpt-4, gpt-3.5-turbo
# Anthropic: claude-3-opus-20240229, claude-3-sonnet-20240229
# Local: your-model-name (e.g., llama2, mistral)
LLM_MODEL=gpt-4-turbo-preview

# LLM API Configuration
# LLM_API_KEY_ENV=OPENAI_API_KEY  # Environment variable name for API key
# LLM_BASE_URL=http://localhost:11434/v1  # For local models (Ollama, vLLM)
# LLM_TEMPERATURE=0.7  # Sampling temperature (0.0-2.0)
# LLM_MAX_TOKENS=2000  # Maximum tokens to generate
# LLM_ENABLE_CACHE=true  # Enable response caching
# LLM_CACHE_TTL=86400  # Cache TTL in seconds (24 hours)
# LLM_MAX_RETRIES=3  # Maximum retry attempts
# LLM_TIMEOUT=60  # Request timeout in seconds

# LLM Cost Control
# LLM_MAX_COST_PER_REQUEST=0.10  # Maximum cost per request (USD)
# LLM_MAX_COST_PER_DAY=100.0  # Maximum daily cost (USD)

# ===== 安全提示 =====
# 1. 永远不要将 .env 文件提交到 Git
# 2. 在 CI/CD 中使用加密的环境变量或密钥管理服务
# 3. 定期轮换 API Tokens
# 4. 生产环境使用更严格的访问控制
# 5. LLM API Keys 应该被视为高度敏感信息
# 6. 使用 config/llm.yaml.example 作为配置模板参考
