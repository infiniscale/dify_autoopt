# å•èŠ‚ç‚¹æå–æ“ä½œæŒ‡å—

## ğŸ“‹ ç›®å½•
1. [åŸºæœ¬ç”¨æ³•](#åŸºæœ¬ç”¨æ³•)
2. [ä»Workflowä¸­æå–ç‰¹å®šèŠ‚ç‚¹](#ä»workflowä¸­æå–ç‰¹å®šèŠ‚ç‚¹)
3. [æ‰‹åŠ¨æ„é€ èŠ‚ç‚¹æå–](#æ‰‹åŠ¨ï¿½ï¿½ï¿½é€ èŠ‚ç‚¹æå–)
4. [å®Œæ•´ç¤ºä¾‹](#å®Œæ•´ç¤ºä¾‹)
5. [å¸¸è§åœºæ™¯](#å¸¸è§åœºæ™¯)

---

## ğŸ”§ åŸºæœ¬ç”¨æ³•

### æ–¹æ³•ç­¾å

```python
def extract_from_node(
    self,
    node: Dict[str, Any],      # èŠ‚ç‚¹å­—å…¸
    workflow_id: str           # æ‰€å±workflow ID
) -> Optional[Prompt]:         # è¿”å›Promptå¯¹è±¡æˆ–None
```

**å‚æ•°è¯´æ˜ï¼š**
- `node`: ä»workflow DSLä¸­æå–çš„å•ä¸ªèŠ‚ç‚¹å­—å…¸
- `workflow_id`: è¯¥èŠ‚ç‚¹æ‰€å±çš„workflowæ ‡è¯†ç¬¦

**è¿”å›å€¼ï¼š**
- å¦‚æœæ˜¯LLMèŠ‚ç‚¹ï¼šè¿”å›`Prompt`å¯¹è±¡
- å¦‚æœä¸æ˜¯LLMèŠ‚ç‚¹ï¼šè¿”å›`None`

---

## ğŸ“¦ ä»Workflowä¸­æå–ç‰¹å®šèŠ‚ç‚¹

### åœºæ™¯1: æå–æŒ‡å®šIDçš„èŠ‚ç‚¹

```python
from src.optimizer import PromptExtractor
import yaml

# 1. åŠ è½½workflow DSL
with open("workflow.yaml", "r", encoding="utf-8") as f:
    workflow_dsl = yaml.safe_load(f)

# 2. åˆ›å»ºæå–å™¨
extractor = PromptExtractor()

# 3. æ‰¾åˆ°æ‰€æœ‰èŠ‚ç‚¹
nodes = workflow_dsl["graph"]["nodes"]

# 4. æ ¹æ®node_idæ‰¾åˆ°ç‰¹å®šèŠ‚ç‚¹
target_node_id = "llm_1"
target_node = None

for node in nodes:
    if node.get("id") == target_node_id:
        target_node = node
        break

# 5. æå–è¿™ä¸ªèŠ‚ç‚¹çš„prompt
if target_node:
    prompt = extractor.extract_from_node(target_node, "wf_001")

    if prompt:
        print(f"æˆåŠŸæå–prompt:")
        print(f"  ID: {prompt.id}")
        print(f"  æ–‡æœ¬é•¿åº¦: {len(prompt.text)}å­—ç¬¦")
        print(f"  å˜é‡æ•°: {len(prompt.variables)}")
        print(f"  å†…å®¹é¢„è§ˆ: {prompt.text[:100]}...")
    else:
        print(f"èŠ‚ç‚¹ {target_node_id} ä¸æ˜¯LLMèŠ‚ç‚¹æˆ–æå–å¤±è´¥")
else:
    print(f"æœªæ‰¾åˆ°èŠ‚ç‚¹ {target_node_id}")
```

### åœºæ™¯2: æå–ç¬¬Nä¸ªLLMèŠ‚ç‚¹

```python
# åªæå–ç¬¬ä¸€ä¸ªLLMèŠ‚ç‚¹
def extract_first_llm_node(workflow_dsl, workflow_id):
    extractor = PromptExtractor()
    nodes = workflow_dsl["graph"]["nodes"]

    for node in nodes:
        # å°è¯•æå–ï¼Œå¦‚æœæ˜¯LLMèŠ‚ç‚¹ä¼šè¿”å›Promptå¯¹è±¡
        prompt = extractor.extract_from_node(node, workflow_id)
        if prompt:
            return prompt  # è¿”å›ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„LLM prompt

    return None  # æ²¡æ‰¾åˆ°ä»»ä½•LLMèŠ‚ç‚¹

# ä½¿ç”¨
first_llm_prompt = extract_first_llm_node(workflow_dsl, "wf_001")
if first_llm_prompt:
    print(f"ç¬¬ä¸€ä¸ªLLMèŠ‚ç‚¹: {first_llm_prompt.node_id}")
```

### åœºæ™¯3: æŒ‰æ¡ä»¶ç­›é€‰èŠ‚ç‚¹

```python
# æå–æ‰€æœ‰åŒ…å«ç‰¹å®šå˜é‡çš„LLMèŠ‚ç‚¹
def extract_nodes_with_variable(workflow_dsl, workflow_id, variable_name):
    extractor = PromptExtractor()
    nodes = workflow_dsl["graph"]["nodes"]
    prompts_with_var = []

    for node in nodes:
        prompt = extractor.extract_from_node(node, workflow_id)
        if prompt and variable_name in prompt.variables:
            prompts_with_var.append(prompt)

    return prompts_with_var

# ä½¿ç”¨ï¼šæ‰¾å‡ºæ‰€æœ‰ä½¿ç”¨äº† {{user_input}} å˜é‡çš„prompts
prompts = extract_nodes_with_variable(
    workflow_dsl,
    "wf_001",
    "user_input"
)
print(f"æ‰¾åˆ° {len(prompts)} ä¸ªä½¿ç”¨ user_input å˜é‡çš„prompts")
```

---

## ğŸ¨ æ‰‹åŠ¨æ„é€ èŠ‚ç‚¹æå–

### åœºæ™¯4: ç›´æ¥ä¼ å…¥èŠ‚ç‚¹æ•°æ®

å¦‚æœä½ å·²ç»æœ‰äº†èŠ‚ç‚¹çš„æ•°æ®ç»“æ„ï¼Œå¯ä»¥ç›´æ¥æå–ï¼š

```python
from src.optimizer import PromptExtractor

extractor = PromptExtractor()

# æ‰‹åŠ¨æ„é€ ä¸€ä¸ªèŠ‚ç‚¹å­—å…¸ï¼ˆç¬¦åˆDify DSLæ ¼å¼ï¼‰
node = {
    "id": "llm_1",
    "data": {
        "type": "llm",
        "title": "LLMèŠ‚ç‚¹",
        "model": {
            "provider": "openai",
            "name": "gpt-4"
        },
        "prompt_template": [
            {
                "role": "system",
                "text": "You are a helpful assistant specializing in {{domain}}."
            },
            {
                "role": "user",
                "text": "Please help me with: {{user_input}}"
            }
        ],
        "temperature": 0.7,
        "max_tokens": 2000
    }
}

# æå–prompt
prompt = extractor.extract_from_node(node, "custom_workflow")

if prompt:
    print(f"æå–æˆåŠŸï¼")
    print(f"Prompt ID: {prompt.id}")
    print(f"æ–‡æœ¬: {prompt.text}")
    print(f"å˜é‡: {prompt.variables}")  # ['domain', 'user_input']
    print(f"è§’è‰²: {prompt.role}")       # 'system'
```

---

## ğŸ’¼ å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹1: äº¤äº’å¼é€‰æ‹©è¦ä¼˜åŒ–çš„ï¿½ï¿½ï¿½ç‚¹

```python
from src.optimizer import PromptExtractor, OptimizerService
import yaml

def interactive_node_optimization():
    """äº¤äº’å¼é€‰æ‹©å¹¶ä¼˜åŒ–ç‰¹å®šèŠ‚ç‚¹"""

    # 1. åŠ è½½workflow
    with open("workflow.yaml", "r") as f:
        workflow_dsl = yaml.safe_load(f)

    workflow_id = workflow_dsl.get("id", "unknown")
    nodes = workflow_dsl["graph"]["nodes"]

    # 2. æ‰¾å‡ºæ‰€æœ‰LLMèŠ‚ç‚¹
    extractor = PromptExtractor()
    llm_prompts = []

    print("æ­£åœ¨æ‰«æLLMèŠ‚ç‚¹...")
    for idx, node in enumerate(nodes):
        prompt = extractor.extract_from_node(node, workflow_id)
        if prompt:
            llm_prompts.append((idx, node, prompt))

    # 3. æ˜¾ç¤ºå¯é€‰çš„LLMèŠ‚ç‚¹
    print(f"\næ‰¾åˆ° {len(llm_prompts)} ä¸ªLLMèŠ‚ç‚¹ï¼š\n")
    for i, (idx, node, prompt) in enumerate(llm_prompts):
        print(f"{i+1}. {prompt.node_id}")
        print(f"   æ–‡æœ¬é¢„è§ˆ: {prompt.text[:80]}...")
        print(f"   å˜é‡æ•°: {len(prompt.variables)}")
        print()

    # 4. è®©ç”¨æˆ·é€‰æ‹©
    choice = int(input("è¯·é€‰æ‹©è¦ä¼˜åŒ–çš„èŠ‚ç‚¹ç¼–å· (1-{}): ".format(len(llm_prompts))))
    selected_idx, selected_node, selected_prompt = llm_prompts[choice - 1]

    # 5. ä¼˜åŒ–è¿™ä¸ªèŠ‚ç‚¹
    print(f"\næ­£åœ¨ä¼˜åŒ–èŠ‚ç‚¹ {selected_prompt.node_id}...")
    service = OptimizerService()

    result = service.optimize_single_prompt(
        prompt=selected_prompt,
        strategy="auto"
    )

    # 6. æ˜¾ç¤ºç»“æœ
    print("\nä¼˜åŒ–å®Œï¿½ï¿½ï¿½ï¼")
    print(f"åŸå§‹prompt:\n{result.original_prompt}\n")
    print(f"ä¼˜åŒ–å:\n{result.optimized_prompt}\n")
    print(f"æ”¹è¿›åˆ†æ•°: {result.improvement_score:.1f}")
    print(f"ç½®ä¿¡åº¦: {result.confidence:.2%}")
    print(f"å˜æ›´è¯´æ˜:")
    for change in result.changes:
        print(f"  - {change.description}")

# è¿è¡Œ
interactive_node_optimization()
```

**è¿è¡Œç¤ºä¾‹ï¼š**
```
æ­£åœ¨æ‰«æLLMèŠ‚ç‚¹...

æ‰¾åˆ° 3 ä¸ªLLMèŠ‚ç‚¹ï¼š

1. llm_1
   æ–‡æœ¬é¢„è§ˆ: You are a customer service assistant. Help users with their inquiries...
   å˜é‡æ•°: 2

2. llm_2
   æ–‡æœ¬é¢„è§ˆ: Analyze the sentiment of the following text and classify it as positive, ne...
   å˜é‡æ•°: 1

3. llm_3
   æ–‡æœ¬é¢„è§ˆ: Generate a summary of: {{document_text}}
   å˜é‡æ•°: 1

è¯·é€‰æ‹©è¦ä¼˜åŒ–çš„èŠ‚ç‚¹ç¼–å· (1-3): 1

æ­£åœ¨ä¼˜åŒ–èŠ‚ç‚¹ llm_1...

ä¼˜åŒ–å®Œæˆï¼
åŸå§‹prompt:
You are a customer service assistant. Help users with their inquiries.

ä¼˜åŒ–å:
You are a professional customer service assistant specializing in providing clear,
helpful responses. Your role is to assist users with {{inquiry_type}} inquiries,
ensuring accuracy and empathy in every interaction.

æ”¹è¿›åˆ†æ•°: 12.5
ç½®ä¿¡åº¦: 85.00%
å˜æ›´è¯´æ˜:
  - Added role clarification and specialization
  - Integrated variable {{inquiry_type}} for context
  - Enhanced professionalism and tone
```

### ç¤ºä¾‹2: æ‰¹é‡å¤„ç†ä½†åˆ†åˆ«ä¼˜åŒ–

```python
def optimize_nodes_separately(workflow_dsl, workflow_id):
    """
    æå–æ‰€æœ‰LLMèŠ‚ç‚¹ï¼Œä½†åˆ†åˆ«ä¼˜åŒ–æ¯ä¸ªèŠ‚ç‚¹
    ï¼ˆä¸å…¨é‡ä¼˜åŒ–ä¸åŒï¼Œè¿™é‡Œå¯ä»¥ä¸ºæ¯ä¸ªèŠ‚ç‚¹ä½¿ç”¨ä¸åŒçš„ç­–ç•¥ï¼‰
    """
    extractor = PromptExtractor()
    service = OptimizerService()

    nodes = workflow_dsl["graph"]["nodes"]
    results = []

    for node in nodes:
        # å•ç‹¬æå–æ¯ä¸ªèŠ‚ç‚¹
        prompt = extractor.extract_from_node(node, workflow_id)

        if not prompt:
            continue

        # æ ¹æ®èŠ‚ç‚¹ç‰¹å¾é€‰æ‹©ä¸åŒç­–ç•¥
        if "sentiment" in prompt.text.lower():
            strategy = "clarity_focus"  # æƒ…æ„Ÿåˆ†æéœ€è¦æ¸…æ™°
        elif "summary" in prompt.text.lower():
            strategy = "efficiency_focus"  # æ‘˜è¦éœ€è¦ç®€æ´
        else:
            strategy = "auto"  # å…¶ä»–è‡ªåŠ¨é€‰æ‹©

        print(f"ä¼˜åŒ– {prompt.node_id} ä½¿ç”¨ç­–ç•¥: {strategy}")

        # ä¼˜åŒ–
        result = service.optimize_single_prompt(prompt, strategy)
        results.append({
            "node_id": prompt.node_id,
            "strategy": strategy,
            "result": result
        })

    return results

# ä½¿ç”¨
results = optimize_nodes_separately(workflow_dsl, "wf_001")

for item in results:
    print(f"\nèŠ‚ç‚¹: {item['node_id']}")
    print(f"ç­–ç•¥: {item['strategy']}")
    print(f"æ”¹è¿›: {item['result'].improvement_score:.1f}åˆ†")
```

---

## ï¿½ï¿½ï¿½ï¿½ å¸¸è§åœºæ™¯

### åœºæ™¯A: åªä¼˜åŒ–å…³é”®èŠ‚ç‚¹

```python
# å®šä¹‰å…³é”®èŠ‚ç‚¹åˆ—è¡¨
CRITICAL_NODES = ["llm_main", "llm_classifier", "llm_summarizer"]

def optimize_critical_nodes_only(workflow_dsl, workflow_id):
    extractor = PromptExtractor()
    service = OptimizerService()

    nodes = workflow_dsl["graph"]["nodes"]
    patches = []

    for node in nodes:
        node_id = node.get("id")

        # åªå¤„ç†å…³é”®èŠ‚ç‚¹
        if node_id not in CRITICAL_NODES:
            continue

        prompt = extractor.extract_from_node(node, workflow_id)
        if prompt:
            result = service.optimize_single_prompt(prompt)
            # ç”Ÿæˆpatch...
            patches.append(result)

    return patches
```

### åœºæ™¯B: æ ¹æ®èŠ‚ç‚¹ä½ç½®æå–

```python
# åªä¼˜åŒ–workflowå¼€å¤´å’Œç»“å°¾çš„èŠ‚ç‚¹
def optimize_boundary_nodes(workflow_dsl, workflow_id):
    extractor = PromptExtractor()
    nodes = workflow_dsl["graph"]["nodes"]

    # æå–ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªLLMèŠ‚ç‚¹
    first_llm = None
    last_llm = None

    for node in nodes:
        prompt = extractor.extract_from_node(node, workflow_id)
        if prompt:
            if first_llm is None:
                first_llm = prompt
            last_llm = prompt  # ä¸æ–­æ›´æ–°ï¼Œæœ€åä¸€ä¸ªå°±æ˜¯æœ«å°¾

    return first_llm, last_llm
```

### åœºæ™¯C: æŒ‰è´¨é‡åˆ†æ•°æå–

```python
def extract_low_quality_nodes(workflow_dsl, workflow_id, threshold=70):
    """åªæå–ä½è´¨é‡çš„èŠ‚ç‚¹"""
    extractor = PromptExtractor()
    analyzer = PromptAnalyzer()

    nodes = workflow_dsl["graph"]["nodes"]
    low_quality_prompts = []

    for node in nodes:
        prompt = extractor.extract_from_node(node, workflow_id)
        if prompt:
            # åˆ†æè´¨é‡
            analysis = analyzer.analyze_prompt(prompt)

            if analysis.overall_score < threshold:
                low_quality_prompts.append({
                    "prompt": prompt,
                    "score": analysis.overall_score,
                    "issues": analysis.issues
                })

    # æŒ‰åˆ†æ•°æ’åºï¼ˆæœ€å·®çš„åœ¨å‰ï¼‰
    low_quality_prompts.sort(key=lambda x: x["score"])

    return low_quality_prompts

# ä½¿ç”¨
low_quality = extract_low_quality_nodes(workflow_dsl, "wf_001", threshold=75)

print(f"å‘ç° {len(low_quality)} ä¸ªä½è´¨é‡èŠ‚ç‚¹ï¼š")
for item in low_quality:
    print(f"  {item['prompt'].node_id}: {item['score']:.1f}åˆ†")
    print(f"    é—®é¢˜: {[i.description for i in item['issues'][:3]]}")
```

---

## ğŸ“Š èŠ‚ç‚¹æ•°æ®ç»“æ„å‚è€ƒ

### DifyèŠ‚ç‚¹çš„å…¸å‹ç»“æ„

```python
# LLMèŠ‚ç‚¹ç¤ºä¾‹
{
    "id": "llm_1",
    "data": {
        "type": "llm",
        "title": "LLMèŠ‚ç‚¹åç§°",
        "model": {
            "provider": "openai",
            "name": "gpt-4",
            "mode": "chat",
            "completion_params": {
                "temperature": 0.7,
                "max_tokens": 2000
            }
        },
        "prompt_template": [
            {
                "role": "system",
                "text": "You are a {{role}}."
            },
            {
                "role": "user",
                "text": "{{user_input}}"
            }
        ]
    }
}

# Question ClassifierèŠ‚ç‚¹ç¤ºä¾‹
{
    "id": "classifier_1",
    "data": {
        "type": "question-classifier",
        "title": "é—®é¢˜åˆ†ç±»å™¨",
        "classes": [...],
        "query_variable_selector": ["sys", "query"],
        "model": {...}
    }
}

# æ¡ä»¶èŠ‚ç‚¹ç¤ºä¾‹ï¼ˆå¸¦system_promptï¼‰
{
    "id": "ifelse_1",
    "data": {
        "type": "if-else",
        "title": "æ¡ä»¶åˆ¤æ–­",
        "system_prompt": "Evaluate if {{condition}} is met",
        "cases": [...]
    }
}
```

---

## âš™ï¸ APIå‚è€ƒ

### extract_from_node() å®Œæ•´å‚æ•°

```python
def extract_from_node(
    node: Dict[str, Any],
    workflow_id: str
) -> Optional[Prompt]:
    """
    Args:
        node: èŠ‚ç‚¹å­—å…¸ï¼ŒåŒ…å«:
            - id: èŠ‚ç‚¹ID (å¿…éœ€)
            - data: èŠ‚ç‚¹æ•°æ® (å¿…éœ€)
                - type: èŠ‚ç‚¹ç±»å‹ (å¿…éœ€)
                - prompt_template: promptæ¨¡æ¿ (LLMèŠ‚ç‚¹å¿…éœ€)
                - å…¶ä»–é…ç½®...

        workflow_id: workflowæ ‡è¯†ç¬¦

    Returns:
        Promptå¯¹è±¡ï¼ˆå¦‚æœæ˜¯LLMèŠ‚ç‚¹ï¼‰
        Noneï¼ˆå¦‚æœä¸æ˜¯LLMèŠ‚ç‚¹æˆ–æå–å¤±è´¥ï¼‰

    Raises:
        æ— ï¼ˆæ‰€æœ‰å¼‚å¸¸éƒ½è¢«æ•è·å¹¶è¿”å›Noneï¼‰
    """
```

### æå–åçš„Promptå¯¹è±¡

```python
Prompt(
    id="wf_001_llm_1",           # workflow_id + node_id
    workflow_id="wf_001",         # æ‰€å±workflow
    node_id="llm_1",              # èŠ‚ç‚¹ID
    node_type="llm",              # èŠ‚ç‚¹ç±»å‹
    text="You are...",            # å®Œæ•´promptæ–‡æœ¬
    role="system",                # è§’è‰²ï¼ˆsystem/user/assistantï¼‰
    variables=["var1", "var2"],   # æå–çš„å˜é‡åˆ—è¡¨
    context={                     # ä¸Šä¸‹æ–‡ä¿¡æ¯
        "model": "gpt-4",
        "temperature": 0.7
    },
    extracted_at=datetime.now()   # æå–æ—¶é—´
)
```

---

## ğŸ¯ æ€»ç»“

### å•èŠ‚ç‚¹æå–çš„ä¸‰ç§æ–¹å¼

| æ–¹å¼ | ä½¿ç”¨åœºæ™¯ | ä»£ç ç¤ºä¾‹ |
|------|---------|---------|
| **ä»workflowä¸­æå–** | å·²æœ‰workflow DSL | `extractor.extract_from_node(nodes[0], "wf_001")` |
| **æ‰‹åŠ¨æ„é€ ** | è‡ªå®šä¹‰èŠ‚ç‚¹æ•°æ® | `extractor.extract_from_node(custom_node, "wf_001")` |
| **æ¡ä»¶ç­›é€‰** | æå–ç¬¦åˆæ¡ä»¶çš„ | éå†nodeså¹¶æŒ‰æ¡ä»¶filter |

### æ ¸å¿ƒè¦ç‚¹

1. âœ… `extract_from_node()` åªå¤„ç†**å•ä¸ªèŠ‚ç‚¹**
2. âœ… éLLMèŠ‚ç‚¹è¿”å› `None`ï¼ˆè‡ªåŠ¨è·³è¿‡ï¼‰
3. âœ… éœ€è¦æ‰‹åŠ¨éå†nodesæ•°ç»„æ¥æå–å¤šä¸ª
4. âœ… å¯ä»¥ç»“åˆæ¡ä»¶å®ç°çµæ´»çš„æå–ç­–ç•¥
5. âœ… è¿”å›çš„Promptå¯¹è±¡å¯ä»¥ç›´æ¥ç”¨äºä¼˜åŒ–

### ä¸å…¨é‡æå–çš„å¯¹æ¯”

```python
# å…¨é‡æå–ï¼ˆè‡ªåŠ¨éå†ï¼‰
prompts = extractor.extract_from_workflow(workflow_dsl, "wf_001")
# å†…éƒ¨è‡ªåŠ¨éå†æ‰€æœ‰èŠ‚ç‚¹ï¼Œè¿”å›æ‰€æœ‰LLM prompts

# å•èŠ‚ç‚¹æå–ï¼ˆæ‰‹åŠ¨æ§åˆ¶ï¼‰
for node in workflow_dsl["graph"]["nodes"]:
    prompt = extractor.extract_from_node(node, "wf_001")
    if prompt:
        # å¤„ç†è¿™ä¸ªprompt
        ...
```

---

**ç”Ÿæˆæ—¶é—´**: 2025-11-18
**é€‚ç”¨ç‰ˆæœ¬**: Optimizer v1.0
